{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86946,"databundleVersionId":10131489,"sourceType":"competition"},{"sourceId":10272092,"sourceType":"datasetVersion","datasetId":6355668},{"sourceId":10272104,"sourceType":"datasetVersion","datasetId":6355678},{"sourceId":10297808,"sourceType":"datasetVersion","datasetId":6373855},{"sourceId":10298407,"sourceType":"datasetVersion","datasetId":6374257}],"dockerImageVersionId":30823,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T16:11:59.363250Z","iopub.execute_input":"2024-12-28T16:11:59.363497Z","iopub.status.idle":"2024-12-28T16:11:59.701950Z","shell.execute_reply.started":"2024-12-28T16:11:59.363477Z","shell.execute_reply":"2024-12-28T16:11:59.701071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ss = pd.read_csv(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/sample_submission.csv\")\nss.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:02:45.642975Z","iopub.execute_input":"2024-12-25T21:02:45.643409Z","iopub.status.idle":"2024-12-25T21:02:45.669751Z","shell.execute_reply.started":"2024-12-25T21:02:45.643385Z","shell.execute_reply":"2024-12-25T21:02:45.668942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\n# Load the datasets from parquet files\ntrain_df = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet')\ntest_df = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet')\n\n# Prepare training dataset for classification\ntrain_samples = []\nfor _, row in train_df.iterrows():\n    train_samples.append({\n        'prompt': row['prompt'],\n        'response': row['response_a'],\n        'winner': 0 if row['winner'] == 'model_a' else 1\n    })\n    train_samples.append({\n        'prompt': row['prompt'],\n        'response': row['response_b'],\n        'winner': 1 if row['winner'] == 'model_b' else 0\n    })\n\n# Convert to a Hugging Face Dataset\ntrain_dataset = Dataset.from_dict({\n    'prompt': [sample['prompt'] for sample in train_samples],\n    'response': [sample['response'] for sample in train_samples],\n    'winner': [sample['winner'] for sample in train_samples],\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:02:48.461884Z","iopub.execute_input":"2024-12-25T21:02:48.462154Z","iopub.status.idle":"2024-12-25T21:02:59.403635Z","shell.execute_reply.started":"2024-12-25T21:02:48.462133Z","shell.execute_reply":"2024-12-25T21:02:59.402939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:03:03.453502Z","iopub.execute_input":"2024-12-25T21:03:03.453819Z","iopub.status.idle":"2024-12-25T21:03:03.464346Z","shell.execute_reply.started":"2024-12-25T21:03:03.453758Z","shell.execute_reply":"2024-12-25T21:03:03.463453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[\"language\"].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:03:07.049526Z","iopub.execute_input":"2024-12-25T21:03:07.049948Z","iopub.status.idle":"2024-12-25T21:03:07.063268Z","shell.execute_reply.started":"2024-12-25T21:03:07.049911Z","shell.execute_reply":"2024-12-25T21:03:07.062467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:03:10.420078Z","iopub.execute_input":"2024-12-25T21:03:10.420388Z","iopub.status.idle":"2024-12-25T21:03:10.4291Z","shell.execute_reply.started":"2024-12-25T21:03:10.420361Z","shell.execute_reply":"2024-12-25T21:03:10.428284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use a subset of the dataset for quick testing\n#train_dataset = train_dataset.select(range(2000))  # Select first 1000 examples","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:03:13.048917Z","iopub.execute_input":"2024-12-25T21:03:13.049302Z","iopub.status.idle":"2024-12-25T21:03:13.054379Z","shell.execute_reply.started":"2024-12-25T21:03:13.049271Z","shell.execute_reply":"2024-12-25T21:03:13.053468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nimport torch.nn.utils.weight_norm\nimport torch.nn as nn\n\nclass WeightNormModel(AutoModelForSequenceClassification):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Applying weight normalization on the classifier layer\n        self.classifier = torch.nn.utils.weight_norm(self.classifier)\n\n# Load your model with weight normalization\n#model = WeightNormModel.from_pretrained(\n    #\"/kaggle/input/bert-model/bert_base_uncased_model\",\n    #num_labels=2,\n#)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"/kaggle/input/model-ml/Model_ml_bert\",  # You can choose other model variants as well\n    num_labels=2,\n    id2label={0: \"model_a\", 1: \"model_b\"},  # Labels corresponding to models\n    label2id={\"model_a\": 0, \"model_b\": 1},\n)\n\n# Freeze all parameters in the base model\nfor param in model.base_model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the last 3 layers of the transformer\nnum_layers = len(model.base_model.encoder.layer)\nfor layer in model.base_model.encoder.layer[num_layers-3:]:\n    for param in layer.parameters():\n        param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:03:17.162794Z","iopub.execute_input":"2024-12-25T21:03:17.163083Z","iopub.status.idle":"2024-12-25T21:03:22.691926Z","shell.execute_reply.started":"2024-12-25T21:03:17.163061Z","shell.execute_reply":"2024-12-25T21:03:22.691079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Number of parameters that require gradients: {total_parameters}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:03:28.006698Z","iopub.execute_input":"2024-12-25T21:03:28.007278Z","iopub.status.idle":"2024-12-25T21:03:28.013005Z","shell.execute_reply.started":"2024-12-25T21:03:28.007247Z","shell.execute_reply":"2024-12-25T21:03:28.011988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\nfrom transformers import AutoTokenizer\nfrom datasets import DatasetDict\nimport torch\nimport random\n\n# Set random seed for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# Ensure deterministic behavior\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/token-ml/tokenzer_ml_bert\")\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(examples['prompt'], examples['response'], return_tensors='pt', padding=True, truncation=True)\n\n# Tokenize the dataset\ntokenized_train = train_dataset.map(\n    lambda examples: {\n        **tokenize_function(examples),\n        \"labels\": examples[\"winner\"],  # Replace 'label' with the actual column name in your dataset\n    },\n    batched=True,\n)\n\n# Split the tokenized_train dataset into training and evaluation datasets\nsplit_datasets = tokenized_train.train_test_split(test_size=0.2, seed=42)  # Adjust test_size as needed\ntokenized_train = split_datasets[\"train\"]\ntokenized_eval = split_datasets[\"test\"]\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return {\"accuracy\": (predictions == labels).mean()}\n\n\n# Prepare training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./model_output\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=150,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"none\",  # Disable logging to WandB\n)\n\n# Data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_eval,  # Provide a validation dataset\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,  # Add compute_metrics\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T21:03:35.776174Z","iopub.execute_input":"2024-12-25T21:03:35.776457Z","execution_failed":"2024-12-25T21:09:41.937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare test dataset for predictions\ntest_samples = []\nfor _, row in test_df.iterrows():\n    test_samples.append({\n        'id': row['id'],\n        'prompt': row['prompt'],\n        'response': row['response_a'],\n    })\n    test_samples.append({\n        'id': row['id'],\n        'prompt': row['prompt'],\n        'response': row['response_b'],\n    })\n\n# Convert to a Hugging Face Dataset\ntest_dataset = Dataset.from_dict({\n    'id': [sample['id'] for sample in test_samples],\n    'prompt': [sample['prompt'] for sample in test_samples],\n    'response': [sample['response'] for sample in test_samples],\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize the test dataset\ntokenized_test = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Make predictions\npredictions = trainer.predict(tokenized_test)\npredicted_labels = np.argmax(predictions.predictions, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare submission DataFrame\nsubmission_data = []\nfor i in range(len(tokenized_test)):\n    # The ID from the test dataset\n    sample_id = tokenized_test['id'][i]  \n    # Determining the winner based on the predicted label\n    winner = 'model_a' if predicted_labels[i] == 0 else 'model_b'\n    \n    # Append to submission data\n    submission_data.append({\n        'id': sample_id,\n        'winner': winner,\n    })\n\n# Create DataFrame\nsubmission_df = pd.DataFrame(submission_data)\n\n# Group by 'id' and take the first predicted winner for each unique id\nsubmission_df = submission_df.groupby('id', as_index=False).first()\n\n# Save as CSV\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}